![Logo](interface.jpg)
## local-AI-Interface
ğŸ“Ÿ local-AI-Interface is a chatbot application powered by a locally hosted AI model, built using Streamlit, LangChain, and DeepSeek. This project provides an interactive user interface for seamless AI conversations without relying on external APIs or cloud services.


## Chatbot Features âœ¨
- ğŸ”’ 100% local execution - no internet required with help of models from Ollama
- ğŸ›ï¸ Model size selection (1.5B or 7B parameters)
- ğŸ’¬ Real-time chat with streaming responses
- ğŸ“š Conversation history persistence
- âš¡ LangChain integration for efficient processing

## ğŸ“Œ Future Enhancements
- ğŸ”„ Multi-Model Support â€“ Compatible with DeepSeek, LLaMA, Mistral, Falcon
- ğŸ¤ Voice Input (Future Feature) â€“ Talk to AI using speech-to-text
- ğŸŒ™ Dark Mode UI (Coming Soon) â€“ A sleek, customizable theme

## Prerequisites ğŸ“‹
- Python 3.8+
- [Ollama](https://ollama.ai/) installed locally
- Deepseek models pulled from Ollama
- 8GB+ RAM (16GB recommended for 7B model)

## Installation âš™ï¸

### 1. Set Up Ollama
```bash
# Start Ollama service (keep running in separate terminal)
ollama serve